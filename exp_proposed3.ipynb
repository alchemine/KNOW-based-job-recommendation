{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73d14ca",
   "metadata": {},
   "source": [
    "# Proposed idea3\n",
    "1. Preprocess `X`\n",
    "2. Label propagation unlabeled `y(=9999999)` using `OPTICS`\n",
    "3. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66221ab2",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b221e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools.common import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df5169",
   "metadata": {},
   "source": [
    "# 2017년 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a8956",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588cbc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train: (7114, 154) (7114,) (7114, 538)\n",
      "- Val: (2372, 154) (2372,) (2372, 538)\n",
      "- Test: (9486, 154)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "train_full_data = pd.read_csv(join(PATH.TRAIN, 'KNOW_2017.csv'), index_col=0)\n",
    "X_test          = pd.read_csv(join(PATH.TEST, 'KNOW_2017_test.csv'), index_col=0)\n",
    "target          = 'knowcode'\n",
    "nlp_cols        = ['bq4_1a', 'bq4_1b', 'bq4_1c', 'bq5_2', 'bq19_1', 'bq30', 'bq31', 'bq32', 'bq33', 'bq34', 'bq38_1']\n",
    "\n",
    "# train_full_ratio   = 0.3\n",
    "# _, train_full_data = train_test_split(train_full_data, stratify=train_full_data[target], test_size=train_full_ratio)\n",
    "\n",
    "train_full_data_ = copy(train_full_data)\n",
    "X_train_full = train_full_data.drop(columns=target)\n",
    "y_train_full = train_full_data[target]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, stratify=y_train_full)\n",
    "\n",
    "oh_enc     = OneHotEncoder(sparse=False)\n",
    "y_train_oh = oh_enc.fit_transform(y_train[:, None])\n",
    "y_val_oh   = oh_enc.transform(y_val[:, None])\n",
    "\n",
    "print(\"- Train:\", X_train.shape, y_train.shape, y_train_oh.shape)\n",
    "print(\"- Val:\", X_val.shape, y_val.shape, y_val_oh.shape)\n",
    "print(\"- Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3831c0",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201f2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fc7e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (7114, 154)\n",
      "y_train (7114,)\n",
      "X_val (2372, 154)\n",
      "y_val (2372,)\n",
      "X_test (9486, 154)\n"
     ]
    }
   ],
   "source": [
    "preprocessor_baseline = get_preprocessor_baseline()\n",
    "data_baseline = dict(\n",
    "    X_train=preprocessor_baseline.fit_transform(X_train),\n",
    "    y_train=y_train,\n",
    "    X_val=preprocessor_baseline.transform(X_val),\n",
    "    y_val=y_val,\n",
    "    X_test=preprocessor_baseline.transform(X_test)\n",
    ")\n",
    "for k, v in data_baseline.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911aba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (7114, 261)\n",
      "y_train (7114,)\n",
      "X_val (2372, 261)\n",
      "y_val (2372,)\n",
      "X_test (9486, 261)\n"
     ]
    }
   ],
   "source": [
    "preprocessor1 = get_preprocessor1()\n",
    "data1 = dict(\n",
    "    X_train=preprocessor1.fit_transform(X_train),\n",
    "    y_train=y_train,\n",
    "    X_val=preprocessor1.transform(X_val),\n",
    "    y_val=y_val,\n",
    "    X_test=preprocessor1.transform(X_test)\n",
    ")\n",
    "for k, v in data1.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c517844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (7114, 261)\n",
      "y_train (7114,)\n",
      "X_val (2372, 261)\n",
      "y_val (2372,)\n",
      "X_test (9486, 261)\n"
     ]
    }
   ],
   "source": [
    "data2 = {}\n",
    "data2['X_train'] = preprocessor1.fit_transform(X_train)\n",
    "data2['y_train'] = preprocess2_y(data2['X_train'], y_train)\n",
    "data2['X_val']   = preprocessor1.transform(X_val)\n",
    "data2['y_val']   = y_val\n",
    "data2['X_test']  = preprocessor1.transform(X_test)\n",
    "for k, v in data1.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c52b4",
   "metadata": {},
   "source": [
    "## 2.3 NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c7bb1",
   "metadata": {},
   "source": [
    "## 2.3.1 Translating\n",
    "## 2.3.2 Embedding\n",
    "**outputs**\n",
    "- last_hidden_state: [n_batches, n_tokens, embedding_dim]\n",
    "- pooler_output: [n_batches, embedding_dim]\n",
    "- hidden_states: [n_layers, n_batches, n_tokens, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6fae02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "class Preprocessor3:\n",
    "    def __init__(self, nlp_cols, selected_nlp_cols, n_components, translate=False):\n",
    "        self.nlp_cols          = nlp_cols\n",
    "        self.selected_nlp_cols = selected_nlp_cols\n",
    "        self.n_components      = n_components\n",
    "        self.translate         = translate\n",
    "        self.imputer           = get_imputer()\n",
    "        if self.translate:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            self.model     = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "            self.model     = BertModel.from_pretrained('bert-base-multilingual-cased', output_hidden_states=True)\n",
    "        self.pcas = {}\n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        X_non_nlp = X_imputed.drop(columns=self.nlp_cols)\n",
    "        X_nlp     = X_imputed[self.selected_nlp_cols]\n",
    "        self.dics = self._get_vector_dics(X_nlp)\n",
    "        X_nlp     = self._allocate_vector(X_nlp, self.dics)\n",
    "#         return X_non_nlp.join(X_nlp)\n",
    "        return X_nlp\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        X_non_nlp = X_imputed.drop(columns=self.nlp_cols)\n",
    "        X_nlp     = X_imputed[self.selected_nlp_cols]\n",
    "        X_nlp     = self._allocate_vector(X_nlp, self.dics)\n",
    "#         return X_non_nlp.join(X_nlp)\n",
    "        return X_nlp\n",
    "    \n",
    "    def _get_vector_dics(self, X):\n",
    "        dics = {}\n",
    "        for col in tqdm(X):\n",
    "            texts = X[col].unique()\n",
    "            tasks = [delayed(self._text2vector)(text, self.tokenizer, self.model, self.translate) for text in texts]\n",
    "            with ProgressBar():\n",
    "                vecs = compute(*tasks)\n",
    "            self.pcas[col] = PCA(n_components=self.n_components, random_state=RANDOM_STATE)\n",
    "            vecs_pca = self.pcas[col].fit_transform(vecs)\n",
    "            dics[col] = dict(zip(texts, vecs_pca))\n",
    "        return dics\n",
    "    \n",
    "    @staticmethod\n",
    "    def _text2vector(text, tokenizer, model, translate):\n",
    "        def text2input(text):\n",
    "            marked_text    = f\"[CLS] {text} [SEP]\"\n",
    "            tokenized_text = tokenizer.tokenize(marked_text)\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            segments_ids   = [1] * len(tokenized_text)\n",
    "            return torch.tensor([indexed_tokens]), torch.tensor([segments_ids])\n",
    "        if translate:\n",
    "            trans = Translator()\n",
    "            text  = trans.translate(text, target='en').text\n",
    "        with torch.no_grad():\n",
    "            outputs = model(*text2input(text))  \n",
    "        last_hidden_state, pooler_output, hidden_states = outputs.values()  # use only hidden_states\n",
    "        token_vecs = hidden_states[-2][0]  # second to last\n",
    "        return token_vecs.mean(axis=0).numpy()\n",
    "    \n",
    "    def _allocate_vector(self, X, dics):\n",
    "        rst = pd.DataFrame(index=X.index)\n",
    "        for col, dic in dics.items():\n",
    "            f        = X[col]\n",
    "            emb_cols = [f\"{col}_{i}\" for i in range(self.pcas[col].n_components_)]\n",
    "            texts    = X[col].unique()\n",
    "            \n",
    "            unknown_texts = [text for text in texts if text not in dic]\n",
    "            if unknown_texts:\n",
    "                tasks = [delayed(self._text2vector)(text, self.tokenizer, self.model, self.translate) for text in unknown_texts]\n",
    "                with ProgressBar():\n",
    "                    unknown_vecs = compute(*tasks)\n",
    "                unknown_vecs_pca = self.pcas[col].transform(unknown_vecs)\n",
    "                dic.update(dict(zip(unknown_texts, unknown_vecs_pca)))\n",
    "            \n",
    "            for text in tqdm(X[col].unique()):\n",
    "                idxs = f[f == text].index\n",
    "                vec  = dic[text]\n",
    "                rst.at[idxs, emb_cols] = vec\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c94fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data3(selected_nlp_cols):\n",
    "    preprocessor3 = Preprocessor3(nlp_cols, selected_nlp_cols, n_components=0.8)\n",
    "    X_train_proc3 = preprocessor3.fit_transform(X_train)\n",
    "    X_val_proc3   = preprocessor3.transform(X_val)\n",
    "    X_test_proc3  = preprocessor3.transform(X_test)\n",
    "\n",
    "    data3 = dict(\n",
    "        X_train=X_train_proc3,\n",
    "        y_train=preprocess2_y(X_train_proc3, y_train),\n",
    "        X_val=X_val_proc3,\n",
    "        y_val=y_val,\n",
    "        X_test=X_test_proc3\n",
    "    )\n",
    "    for k, v in data3.items():\n",
    "        print(k, v.shape)\n",
    "    return data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/7114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 36.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/7114 [00:37<73:16:35, 37.09s/it]\n",
      "100%|██████████| 979/979 [00:00<00:00, 1155.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  9.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405/405 [00:00<00:00, 1132.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 35.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1177/1177 [00:01<00:00, 1118.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (7114, 70)\n",
      "y_train (7114,)\n",
      "X_val (2372, 70)\n",
      "y_val (2372,)\n",
      "X_test (9486, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/7114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 37.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/7114 [00:37<73:55:08, 37.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11min  8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/7114 [11:47<809:09:17, 409.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 39.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/7114 [12:27<475:57:15, 240.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min  0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/7114 [13:28<399:13:03, 202.14s/it]\n",
      "100%|██████████| 979/979 [00:00<00:00, 1103.04it/s]\n",
      " 87%|████████▋ | 4527/5180 [00:57<00:07, 82.14it/s]"
     ]
    }
   ],
   "source": [
    "data3_1 = get_data3(['bq30'])\n",
    "data3_4 = get_data3(['bq30', 'bq31', 'bq32', 'bq33'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b33ed1",
   "metadata": {},
   "source": [
    "# 3. Training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c636ad64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- baseline | Train: 1.00 | Val: 0.41727582904140587\n",
      "- proposed1 | Train: 1.00 | Val: 0.5602074643542937\n",
      "- proposed2 | Train: 1.00 | Val: 0.5677837322466011\n",
      "- proposed3 | Train: 1.00 | Val: 0.5536757924257704\n",
      "CPU times: user 25min 44s, sys: 2min 7s, total: 27min 52s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=700, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "for name, data in zip(['baseline', 'proposed1', 'proposed2', 'proposed3'], [data_baseline, data1, data2, data3]):\n",
    "    X_t, y_t = data['X_train'], data['y_train']\n",
    "    X_v, y_v = data['X_val'], data['y_val']\n",
    "\n",
    "    model.fit(X_t, y_t)\n",
    "    p_t = model.predict(X_t)\n",
    "    p_v = model.predict(X_v)\n",
    "    \n",
    "    if name != 'proposed1':\n",
    "        y_t, y_v = postprocess2_y(y_t), postprocess2_y(y_v)\n",
    "        p_t, p_v = postprocess2_y(p_t), postprocess2_y(p_v)\n",
    "\n",
    "    print(f\"- {name} | Train: {f1_score(y_t, p_t, average='macro'):.2f} | Val: {f1_score(y_v, p_v, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491debe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=700, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "for name, data in zip(['proposed3_1', 'proposed3_4'], [data3_1, data3_4]):\n",
    "    X_t, y_t = data['X_train'], data['y_train']\n",
    "    X_v, y_v = data['X_val'], data['y_val']\n",
    "\n",
    "    model.fit(X_t, y_t)\n",
    "    p_t = model.predict(X_t)\n",
    "    p_v = model.predict(X_v)\n",
    "    \n",
    "    if name != 'proposed1':\n",
    "        y_t, y_v = postprocess2_y(y_t), postprocess2_y(y_v)\n",
    "        p_t, p_v = postprocess2_y(p_t), postprocess2_y(p_v)\n",
    "\n",
    "    print(f\"- {name} | Train: {f1_score(y_t, p_t, average='macro'):.2f} | Val: {f1_score(y_v, p_v, average='macro')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
